{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "1. Rename this file with your name and date of submission E.g., john_doe_yyyymmdd.ipynb\n",
    "2. The csv files include ohlc and volume data from 2024-03-18 to 2024-06-14. For the week of **May 27th** (2024-05-27 to 2024-05-31), identify important prices or ranges for that week. Define \"important\" and provide an explanation. You can use data outside of this range to test hypotheses.\n",
    "3. Analyze the data and create a module to identify this important price.\n",
    "    - (Optional) Create a ranking system to score probability or price importance.\n",
    "4. Feel free to use any resources available.\n",
    "4. Provide a write up below or in a separate document to explain your thought process, hypotheses, and any other interesting points you observed through your analysis.\n",
    "\n",
    "Keep in mind anomalous times including 08:30 EST due to high impact news. 10:00 AM etc. Use this [calendar](https://www.investing.com/economic-calendar/).\n",
    "\n",
    "**Evaluation Metrics**<br>\n",
    "We will assess your submission based on the following three categories:\n",
    "1. Novelty of idea/approach\n",
    "2. Analytical ability and scientific reasoning\n",
    "3. Coding skill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Schema\n",
    "\n",
    "* **ts_event**: time in EST\n",
    "* **open**: price at start of time interval\n",
    "* **high**: max price during time interval\n",
    "* **low**: min price during time interval\n",
    "* **close**: last price of time interval\n",
    "* **volume**: total volume traded (A + B + N)\n",
    "* **volume_A**: total volume traded on A side\n",
    "* **volume_B**: total volume traded on B side\n",
    "* **volume_N**: total volume traded on N side\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_1D = pd.read_csv('1D.csv')\n",
    "df_60min = pd.read_csv('60min.csv')\n",
    "df_5min = pd.read_csv('5min.csv')\n",
    "df_1min = pd.read_csv('1min.csv')\n",
    "df_10s = pd.read_csv('10s.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will begin by creating a function to filter data for a specific date range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_60min['ts_event'] = pd.to_datetime(df_60min['ts_event'])\n",
    "\n",
    "start_date = \"2024-05-27\"\n",
    "end_date = \"2024-05-31 23:59:59\"\n",
    "\n",
    "def filter_data(df, start_date, end_date):\n",
    "    df['ts_event'] = pd.to_datetime(df['ts_event'])\n",
    "    return df[(df['ts_event'] >= start_date) & (df['ts_event'] <= end_date)]\n",
    "\n",
    "# I will create a data frame df_week that only considers the data from the week we are interested in. \n",
    "df_week = filter_data(df_60min, start_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Weekly Range\n",
    "\n",
    "By the values of the prices in the ohlc collumn, I highly suspect that this data is for the ESM2024 futures contract. I will check the highest and lowest point of the weekly high to low and confirm this with the data on Trading View."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest price: 5339.0 reached at 2024-05-28 04:00:00-04:00\n",
      "Lowest price: 5205.5 reached at 2024-05-31 12:00:00-04:00\n"
     ]
    }
   ],
   "source": [
    "highest_price_row = df_week.loc[df_week['high'].idxmax()]\n",
    "highest_price = highest_price_row['high']\n",
    "highest_price_time = highest_price_row['ts_event']\n",
    "\n",
    "lowest_price_row = df_week.loc[df_week['low'].idxmin()]\n",
    "lowest_price = lowest_price_row['low']\n",
    "lowest_price_time = lowest_price_row['ts_event']\n",
    "\n",
    "print(f\"Highest price: {highest_price} reached at {highest_price_time}\")\n",
    "print(f\"Lowest price: {lowest_price} reached at {lowest_price_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the weekly candle traded in a range of 133.5 points from high to low. Indeed, this corresponds with the data available in Trading View, so I can confirm that this data is for ES."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Economic Calendar\n",
    "\n",
    "Before we start any analysis, let's check the economic calendar. Since we know we are looking at ES, I will filter the economic calendar to only include USD news that is high impact. We can see that Monday is a bank holiday, meaning that it is less likely that we will get high quality opportunity to trade (depending on the strategy we use) since there is much less volume during the day. In addition, we can see 10am Consumer Confidence on Tuesday, and GDP / Inital Jobless Claims getting released at 8:30 on Thursday. In general, we want to avoid trading before high impact news releases, as price tends to consolidate and be more lethargic in anticipation for the injection of volatility at those times. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Important Prices\n",
    "\n",
    "We can define 4 different types of price ranges that we can classify as \"important\".\n",
    "\n",
    "1. **High Volume Areas**: Prices where there is a significant amount of trading volume. This often indicates institutional interest or strong participation by market participants. We are interested in these places because it is where price moves more efficiently towards where we anticipate it to go.\n",
    "2. **Support and Resistance Levels**: Prices where the market consistently uses to bounce back up or back down from. These can also be viewed as places where swing highs / swing lows were formed. \n",
    "3. **Time**: We can also consider prices that are traded to during hours where the market is volatile to be more important.\n",
    "4. **VWAP**: I will classify price ranges that are farther away from the weekly VWAP to be \"important\" since they are more likely to be \"overbough\" or \"oversold\".\n",
    "\n",
    "The goal is to create an algorithm that assigns a score to price bins depending on these factors. In order to do so, I will create four helper functions: one for each of these metrics. The goal of each helper function is to output a normalized score from 0 to 1 of their corresponding metric for each price bin. At the end, my rank_prices function will compute a weighted sum of all these scores and output the most \"important\" price bins in order based on these factors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. High Volume Areas\n",
    "\n",
    "This function computes the volume score for each price bin. The score is based on the trading volume at each price bin, normalized between 0 and 1. Higher volume indicates more trading activity and thus increases the score. The score is scaled so that the price bins with the highest volume get a score of 1, while bins with lower volume get a proportionally lower score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_volume_score(df, price_range=5):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Create price bins \n",
    "    df['price_bin'] = (df['close'] // price_range) * price_range\n",
    "    \n",
    "    # Calculate total volume for each price bin\n",
    "    volume_by_bin = df.groupby('price_bin')['volume'].sum().reset_index()\n",
    "    \n",
    "    # Normalize\n",
    "    max_volume = volume_by_bin['volume'].max()\n",
    "    volume_by_bin['volume_score'] = volume_by_bin['volume'] / max_volume\n",
    "    \n",
    "    df = df.merge(volume_by_bin[['price_bin', 'volume_score']], on='price_bin', how='left')\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of this function giving a volume score for each price bin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    price_bin  volume_score\n",
      "0      5315.0      0.138862\n",
      "1      5320.0      0.153214\n",
      "10     5325.0      0.465505\n",
      "11     5330.0      0.018676\n",
      "23     5335.0      0.024100\n"
     ]
    }
   ],
   "source": [
    "volume_score_example = calculate_volume_score(df_week, price_range=5).drop_duplicates(subset=['price_bin'])\n",
    "print(volume_score_example[['price_bin', 'volume_score']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Support and Resistance Levels\n",
    "\n",
    "This function identifies price reversals (swing highs and lows) and assigns a reversal score to each price bin. Price bins with more frequent reversals are given higher scores, as these points indicate key support or resistance levels in the market. The score is normalized between 0 and 1, with higher scores indicating price bins with more reversals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_price_reversals(df, window=1, price_range=5):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Identify swing highs and swing lows\n",
    "    df['swing_high'] = df['high'][(df['high'] > df['high'].shift(window)) & (df['high'] > df['high'].shift(-window))]\n",
    "    df['swing_low'] = df['low'][(df['low'] < df['low'].shift(window)) & (df['low'] < df['low'].shift(-window))]\n",
    "    df['reversal_price'] = df['swing_high'].fillna(df['swing_low'])\n",
    "        \n",
    "    # Create price bins \n",
    "    df['price_bin'] = (df['reversal_price'] // price_range) * price_range\n",
    "    \n",
    "    # Group by price_bin and count the number of reversals in each bin\n",
    "    reversal_scores = df.groupby('price_bin').agg({\n",
    "        'reversal_price': 'count'\n",
    "    }).reset_index()\n",
    "\n",
    "    # Normalize\n",
    "    max_reversals = reversal_scores['reversal_price'].max()\n",
    "    reversal_scores['reversal_score'] = (reversal_scores['reversal_price'] / max_reversals)\n",
    "    \n",
    "    return reversal_scores[['price_bin', 'reversal_score']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of this function giving a reversal score for each price bin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   price_bin  reversal_score\n",
      "0     5205.0             0.2\n",
      "1     5230.0             0.4\n",
      "2     5235.0             0.6\n",
      "3     5245.0             0.6\n",
      "4     5250.0             0.4\n"
     ]
    }
   ],
   "source": [
    "price_reversal_scores = find_price_reversals(df_week, window=1, price_range=5)\n",
    "print(price_reversal_scores.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Time\n",
    "\n",
    "Thirdly, I will classify important price levels as ones that have been traded to during key times of interest. These will be determined based on the historic volatility of the market at every given time of the day. In order to do this, first I display a data frame below that measures the volatility of each hourly candle using the historical data provided based on the standard deviation of the hourly candle range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hour</th>\n",
       "      <th>volatility</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.468255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2.635023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3.666430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3.243747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3.514154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>3.018782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>3.765564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>4.678464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>17.025102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>7.635810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>10.275672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>6.934395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>7.015142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>10.646746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>13.433284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>12.068705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>9.803273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>4.006505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>3.039598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>3.721605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>7.216425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>4.253968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>3.560116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hour  volatility\n",
       "0      0    1.468255\n",
       "1      1    2.635023\n",
       "2      2    3.666430\n",
       "3      3    3.243747\n",
       "4      4    3.514154\n",
       "5      5    3.018782\n",
       "6      6    3.765564\n",
       "7      7    4.678464\n",
       "8      8   17.025102\n",
       "9      9    7.635810\n",
       "10    10   10.275672\n",
       "11    11    6.934395\n",
       "12    12    7.015142\n",
       "13    13   10.646746\n",
       "14    14   13.433284\n",
       "15    15   12.068705\n",
       "16    16    9.803273\n",
       "17    18    4.006505\n",
       "18    19    3.039598\n",
       "19    20    3.721605\n",
       "20    21    7.216425\n",
       "21    22    4.253968\n",
       "22    23    3.560116"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assign a score for the hours that have the best volatility based on all the data provided before May 27th. \n",
    "df_60min['range'] = df_60min['high'] - df_60min['low']  # High-low range as a proxy for volatility\n",
    "hourly_volatility = df_60min[df_60min['ts_event'] < \"2024-05-27\"].groupby(df_60min['ts_event'].dt.hour)['range'].std().reset_index()\n",
    "hourly_volatility.columns = ['hour', 'volatility']\n",
    "hourly_volatility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below generates a time-based volatility score for each price bin. It looks at the volatility of the market during different hours and assigns higher scores to price bins that occur during historically volatile periods. This score is normalized between 0 and 1, with higher values representing price bins that coincide with higher volatility hours. For example, if a price zone is traded inside of at 10:00am, it will receive a much higher score compared to a price level that is traded at 16:00 when the NYSE is closed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_time_score(df, hourly_volatility):\n",
    "    df = df.copy()  \n",
    "    df['time_score'] = 0\n",
    "    \n",
    "    # Assign volatility score based on the hour of the event\n",
    "    df['hour'] = pd.to_datetime(df['ts_event']).dt.hour\n",
    "    \n",
    "    # Volatility score for each hour\n",
    "    df = df.merge(hourly_volatility, on='hour', how='left')\n",
    "    \n",
    "    df['time_score'] = (df['volatility'] / df['volatility'].max())\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "### 4. VWAP\n",
    "\n",
    "Finally, I will create a function to calculate the VWAP of data set given the Close prices and the Volume. To classify important prices this week, I will use the VWAP price for the whole week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VWAP Price for the week: 5278.01147782906\n"
     ]
    }
   ],
   "source": [
    "def calculate_vwap(df):\n",
    "    return (df['close'] * df['volume']).sum() / df['volume'].sum()\n",
    "\n",
    "vwap_price = calculate_vwap(df_week)\n",
    "print(f\"VWAP Price for the week: {vwap_price}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Price Ranking\n",
    "\n",
    "Finally, I defined the rank_prices algorithm below using the previous helper functions. It is designed to assign a total score from 0 to 1 to each price bin based on the following factors in descending order of importance: volume, number of price reversals, time volatility, and distance to VWAP. I assigned the weights of these factors to be 40%, 30%, 20%, and 10%, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rank_prices(df, vwap_price, hourly_volatility, price_range=5):\n",
    "    df = df.copy()  # Copy to avoid warnings\n",
    "    \n",
    "    # Create price bins \n",
    "    df['price_bin'] = (df['close'] // price_range) * price_range\n",
    "    \n",
    "    # Calculate volume score\n",
    "    df = calculate_volume_score(df, price_range = price_range)  # Volume weight\n",
    "    \n",
    "    # Distance to VWAP score\n",
    "    df['distance_to_vwap'] = np.abs(df['price_bin'] - vwap_price)\n",
    "    df['vwap_score'] = (df['distance_to_vwap'] / df['distance_to_vwap'].max())  # VWAP weight\n",
    "    \n",
    "    # Volatility score \n",
    "    df = calculate_time_score(df, hourly_volatility)\n",
    "    \n",
    "    # Reversal points score\n",
    "    reversal_scores = find_price_reversals(df, window=1, price_range=price_range)\n",
    "    \n",
    "    # Merge the reversal scores into the main DataFrame\n",
    "    df = df.merge(reversal_scores[['price_bin', 'reversal_score']], on='price_bin', how='left').fillna(0)\n",
    "    \n",
    "    # Total score combining all factors\n",
    "    df['total_score'] = 0.4 * df['volume_score'] + 0.3 * df['reversal_score'] + 0.2 * df['time_score'] + 0.1 * df['vwap_score']\n",
    "    \n",
    "    # Sort by total score\n",
    "    df = df.groupby('price_bin').agg({'total_score': 'max'}).reset_index()\n",
    "    df = df.sort_values('total_score', ascending=False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the top ranking of the price bins for the week of May 27th 2024 with a score from 0 to 1 for each: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    price_bin  total_score\n",
      "12     5295.0     0.868736\n",
      "18     5325.0     0.520773\n",
      "5      5250.0     0.493168\n",
      "1      5230.0     0.486962\n",
      "2      5235.0     0.464685\n"
     ]
    }
   ],
   "source": [
    "ranked_prices = rank_prices(df_week, vwap_price, hourly_volatility, price_range=5)\n",
    "print(ranked_prices.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
